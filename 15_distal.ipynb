{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6af8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, TextIO, List, Dict, Optional, Any, Callable, Sequence, cast\n",
    "from io import FileIO\n",
    "from dataclasses import dataclass, replace, field\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import more_itertools as itertools\n",
    "from enum import IntEnum\n",
    "from xdsl.dialects import arith, builtin, tensor, linalg, func\n",
    "from xdsl.frontend.pyast.context import PyASTContext, TypeRegistry\n",
    "from xdsl.frontend.pyast.code_generation import CodeGeneration, CodeGenerationVisitor\n",
    "from xdsl import ir\n",
    "from xdsl.irdl import irdl_op_definition, IRDLOperation, prop_def, result_def, operand_def, var_operand_def, attr_def\n",
    "from xdsl import passes\n",
    "import dowhen\n",
    "import isl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "577522f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtin.module {\n",
      "  func.func @matmul(%A : memref<512x2048xf32>, %B : memref<2048x1024xf32>, %C : memref<512x1024xf32>, %m : index, %n : index, %k : index) -> memref<512x1024xf32> {\n",
      "    %0 = \"buffer.access\"(%C, %m, %n) : (memref<512x1024xf32>, index, index) -> f32\n",
      "    %1 = \"buffer.access\"(%B, %k, %n) : (memref<2048x1024xf32>, index, index) -> f32\n",
      "    %2 = \"buffer.access\"(%A, %m, %k) : (memref<512x2048xf32>, index, index) -> f32\n",
      "    %3 = arith.mulf %2, %1 : f32\n",
      "    \"tensor.assign\"(%0, %3) : (f32, f32) -> ()\n",
      "    func.return %C : memref<512x1024xf32>\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class ParameterizedTypeRegistry(TypeRegistry):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self._generic_mapping: dict[type, type] = {}\n",
    "\n",
    "  def resolve_attribute(\n",
    "      self, annotation_name: str, globals: dict[str, Any]\n",
    "  ) -> ir.TypeAttribute | None:\n",
    "    \"\"\"Get an IR type attribute from a string annotation.\"\"\"\n",
    "    annotation = cast(\n",
    "        type,\n",
    "        eval(annotation_name, globals, None),\n",
    "    )\n",
    "    if isinstance(annotation, ir.TypeAttribute):\n",
    "      return annotation\n",
    "    return self._mapping.get(annotation, None)\n",
    "\n",
    "  def get_annotation(self, attribute: ir.TypeAttribute) -> type | None:\n",
    "    anno = super().get_annotation(attribute)\n",
    "    if anno is None:\n",
    "      for key, value in self._generic_mapping.items():\n",
    "        if value == type(attribute):\n",
    "          return key\n",
    "    return anno\n",
    "\n",
    "  def register_param_type(self, annotation: type,\n",
    "                          attributeType: type):\n",
    "    self._generic_mapping[annotation] = attributeType\n",
    "\n",
    "\n",
    "class MyCodeGenVisitor(CodeGenerationVisitor):\n",
    "  def parse_op(self, ir_type, func_name, args: tuple):\n",
    "    ir_type = cast(ir.TypeAttribute, ir_type)\n",
    "    source_type = self.type_converter.type_registry.get_annotation(ir_type)\n",
    "    assert source_type\n",
    "    function_name = f\"{source_type.__qualname__}.{func_name}\"\n",
    "    op = self.type_converter.function_registry.resolve_operation(\n",
    "        module_name=source_type.__module__,\n",
    "        method_name=function_name,\n",
    "        args=args,\n",
    "    )\n",
    "    assert op\n",
    "    self.inserter.insert_op(op)\n",
    "\n",
    "  def visit_Subscript(self, node):\n",
    "    self.visit(node.slice)\n",
    "    elts = [self.inserter.get_operand() for i in range(len(node.slice.elts))]\n",
    "    self.visit(node.value)\n",
    "    value = self.inserter.get_operand()\n",
    "    self.parse_op(value.type, '__getitem__', (value, elts[::-1]))\n",
    "\n",
    "  def visit_Tuple(self, node):\n",
    "    for elt in node.elts:\n",
    "      self.visit(elt)\n",
    "\n",
    "  def visit_Assign(self, node) -> None:\n",
    "    self.visit(node.targets[0])\n",
    "    target = self.inserter.get_operand()\n",
    "    self.visit(node.value)\n",
    "    value = self.inserter.get_operand()\n",
    "    self.inserter.insert_op(AssignOp(target, value))\n",
    "\n",
    "\n",
    "dowhen.when(CodeGeneration.run_with_type_converter, \"+9\").do(\n",
    "    lambda type_converter, module, file:\n",
    "    {\"visitor\": MyCodeGenVisitor(type_converter, module, file)})\n",
    "\n",
    "\n",
    "class IterKind(IntEnum):\n",
    "  Serial = 0\n",
    "  Distributed = 1\n",
    "  Tensorize = 2\n",
    "\n",
    "\n",
    "class UsageKind(IntEnum):\n",
    "  Input = 0\n",
    "  Output = 1\n",
    "  Const = 2\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class IterVar:\n",
    "  name: str\n",
    "  lower_bound: int | None\n",
    "  upper_bound: int | None\n",
    "  step: int = 1\n",
    "\n",
    "  @property\n",
    "  def extent(self) -> int:\n",
    "    return self.upper_bound - self.lower_bound\n",
    "\n",
    "  @staticmethod\n",
    "  def range(name: str, extent: int):\n",
    "    return IterVar(name, 0, extent, 1)\n",
    "\n",
    "  @staticmethod\n",
    "  def symbol(name: str):\n",
    "    vars = tuple(map(lambda s: IterVar(s, None, None, 1), name.split(' ')))\n",
    "    return vars[0] if len(vars) == 1 else vars\n",
    "\n",
    "  def __hash__(self):\n",
    "    return hash((self.name, self.lower_bound, self.upper_bound, self.step))\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.name\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Buffer:\n",
    "  name: str\n",
    "  dims: tuple[int | IterVar, ...]\n",
    "  dtype: type = float\n",
    "  sharding: None | isl.map = None\n",
    "  usage: UsageKind = UsageKind.Input\n",
    "\n",
    "  @property\n",
    "  def shape(self) -> tuple[int | IterVar, ...]:\n",
    "    return tuple(d for d in self.dims)\n",
    "\n",
    "  @property\n",
    "  def domain(self) -> isl.set:\n",
    "    def render(i): return f'0 <= d{i} < {self.dims[i]}' if isinstance(\n",
    "        self.dims[i], int) else f'{self.dims[i].lower_bound} <= d{i} < {self.dims[i].upper_bound}'\n",
    "    return isl.set(f\"{{ {self.name}[{','.join([f'd{i}' for i in range(len(self.dims))])}] : {' and '.join([render(i) for i in range(len(self.dims))])} }}\")\n",
    "\n",
    "  def __hash__(self):\n",
    "    return hash((self.name, self.dims, self.dtype, str(self.sharding)))\n",
    "\n",
    "  @classmethod\n",
    "  def __class_getitem__(cls, args):\n",
    "    elem_type = None\n",
    "    if issubclass(args[0], float):\n",
    "      elem_type = builtin.f32\n",
    "    elif issubclass(args[0], int):\n",
    "      elem_type = builtin.i32\n",
    "    else:\n",
    "      raise TypeError(f\"Unsupported element type: {args[0]}\")\n",
    "    return builtin.MemRefType(elem_type, [builtin.IntAttr(arg) for arg in args[1]])\n",
    "\n",
    "  def __setitem__(cls, args):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def __getitem__(cls, args):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def __iadd__(self, value):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def __matmul__(self, value):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "@irdl_op_definition\n",
    "class AccessOp(IRDLOperation):\n",
    "  name = \"buffer.access\"\n",
    "\n",
    "  buffer = operand_def(builtin.MemRefType)\n",
    "  indices = var_operand_def(builtin.IndexType)\n",
    "  result = result_def(builtin.Attribute)\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      buffer: ir.SSAValue,\n",
    "      indices: Sequence[ir.SSAValue] | ir.SSAValue,\n",
    "      result_type: ir.Attribute,\n",
    "  ):\n",
    "    return super().__init__(operands=[buffer, indices], result_types=[result_type])\n",
    "\n",
    "\n",
    "@irdl_op_definition\n",
    "class AssignOp(IRDLOperation):\n",
    "  name = \"tensor.assign\"\n",
    "\n",
    "  target = operand_def(ir.TypeAttribute)\n",
    "  value = operand_def(ir.TypeAttribute)\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      target: ir.SSAValue,\n",
    "      value: ir.SSAValue,\n",
    "  ):\n",
    "    return super().__init__(operands=[target, value],\n",
    "                            result_types=[])\n",
    "\n",
    "\n",
    "type_registry = ParameterizedTypeRegistry()\n",
    "type_registry.register_param_type(Buffer, builtin.MemRefType)\n",
    "ctx = PyASTContext(type_registry)\n",
    "ctx.register_type(float, builtin.f32)\n",
    "ctx.register_type(IterVar, builtin.IndexType())\n",
    "ctx.register_function(Buffer.__getitem__, lambda *args:\n",
    "                      AccessOp(args[0], args[1], args[0].type.element_type))\n",
    "ctx.register_function(Buffer.__setitem__, lambda *args:\n",
    "                      AssignOp(args[0], args[1]))\n",
    "ctx.register_function(float.__mul__, arith.MulfOp)\n",
    "\n",
    "\n",
    "@ctx.parse_program\n",
    "def matmul(A: Buffer[float, [512, 2048]], B: Buffer[float, [2048, 1024]], C: Buffer[float, [512, 1024]], m: IterVar, n: IterVar, k: IterVar) -> Buffer[float, [512, 1024]]:\n",
    "  C[m, n] = A[m, k] * B[k, n]\n",
    "  return C\n",
    "\n",
    "\n",
    "print(matmul.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64739ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class Mesh:\n",
    "  dims: tuple[IterVar, ...]\n",
    "\n",
    "  @property\n",
    "  def shape(self) -> tuple[int, ...]:\n",
    "    return tuple(var.extent for var in self.dims)\n",
    "\n",
    "  @property\n",
    "  def domain(self) -> isl.set:\n",
    "    return isl.set(f\"{{ Mesh[{','.join(map(str, self.dims))}] : {' and '.join([f'{dim.lower_bound} <= {dim} < {dim.upper_bound}' for dim in self.dims])} }}\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Access:\n",
    "  buffer: Buffer\n",
    "  relation: isl.map\n",
    "\n",
    "  def __hash__(self):\n",
    "    return hash((self.buffer, str(self.relation)))\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Transfer:\n",
    "  access: Access\n",
    "  schedule: isl.map\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Computation:\n",
    "  op: str\n",
    "  domain: isl.set\n",
    "  schedule: isl.map\n",
    "  accesses: List[Access]\n",
    "  iter_vars: tuple[IterVar]\n",
    "  mesh: Mesh | None = None\n",
    "  iter_kinds: dict[IterVar, IterKind] = field(default_factory=dict)\n",
    "  dim_bindings: dict[int, List[Tuple[Access, int]]] = field(default_factory=dict)\n",
    "  transfers: dict[IterVar, Tuple[Transfer, ...]] = field(default_factory=dict)\n",
    "\n",
    "  def name(self):\n",
    "    return self.domain.get_tuple_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237697c",
   "metadata": {},
   "source": [
    "这里对于index notation的解析仅做简单实现，即iterVar直接参与buffer访问的情况，暂时不支持`A[i+1]`或`B[i+j]`这样复杂的模式，因为在对buffer进行sharding调度时，会需要考虑操作迭代域还是buffer的轴，这是一个困难的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20db3c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[k' = k, m' = m, n' = n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), accesses=[Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))], iter_vars=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), mesh=None, iter_kinds={}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IndexCollector:\n",
    "  def __init__(self, iter_set: set[IterVar]):\n",
    "    self.iter_set: set[IterVar] = iter_set\n",
    "\n",
    "  def collect(self, node: ir.IRWithUses):\n",
    "    # todo support complex index pattern\n",
    "    match node:\n",
    "      case ir.BlockArgument():\n",
    "        match node.type:\n",
    "          case builtin.IndexType():\n",
    "            iter_var = IterVar.symbol(node.name_hint)\n",
    "            self.iter_set.add(iter_var)\n",
    "            return iter_var\n",
    "      case _:\n",
    "        raise NotImplementedError(\"Unsupported index node\")\n",
    "\n",
    "\n",
    "class StmtCollector:\n",
    "  def __init__(self, stmt_name: str):\n",
    "    self.stmt_name = stmt_name\n",
    "    self.iter_set: set[IterVar] = set()\n",
    "    self.access_dict: dict[AccessOp, (Buffer, list[ir.Operation])] = {}\n",
    "    self.op: str = None\n",
    "    self.on_value = False\n",
    "\n",
    "  def visit(self, node: ir.IRWithUses):\n",
    "    match node:\n",
    "      case ir.BlockArgument():\n",
    "        match node.type:\n",
    "          case builtin.MemRefType():\n",
    "            return Buffer(node.name_hint, tuple(dim.data for dim in node.type.shape), node.type.element_type, usage=UsageKind.Input if self.on_value else UsageKind.Output)\n",
    "          case _:\n",
    "            return\n",
    "      case ir.SSAValue() | ir.Operation():\n",
    "        op = node if isinstance(node, ir.Operation) else node.owner\n",
    "        match op:\n",
    "          case AssignOp():\n",
    "            self.on_value = False\n",
    "            self.visit(op.target)\n",
    "            self.on_value = True\n",
    "            self.visit(op.value)\n",
    "            self.on_value = False\n",
    "          case AccessOp():\n",
    "            b: Buffer = self.visit(op.buffer)\n",
    "            indices = [IndexCollector(self.iter_set).collect(i) for i in op.indices]\n",
    "            self.access_dict[op] = (b, indices)\n",
    "          case arith.MulfOp():\n",
    "            assert self.op is None\n",
    "            self.op = op.name\n",
    "            self.visit(op.lhs)\n",
    "            self.visit(op.rhs)\n",
    "\n",
    "\n",
    "class PolyhedronExtractPass(passes.ModulePass):\n",
    "  name = \"polyhedron_analysis\"\n",
    "\n",
    "  computations: list[Computation] = []\n",
    "\n",
    "  def analysis_stmt(self, op: AssignOp):\n",
    "    assert len(self.computations) == 0, \"not support stmts more than 1\"\n",
    "    stmt_name = f's{len(self.computations)}'\n",
    "    c = StmtCollector(stmt_name)\n",
    "    c.visit(op)\n",
    "    accesses = []\n",
    "    iters = sorted(c.iter_set, key=lambda i: i.name)\n",
    "    domain_dims = list(map(lambda x: x.name, iters))\n",
    "    domain = isl.set(f\"{{ {stmt_name}[{','.join(domain_dims)}] }}\")\n",
    "    bounded_iters: dict[str, IterVar] = {}\n",
    "    dim_bindings: dict[int, List[Tuple[int, int]]] = {}\n",
    "    for (bf, indices) in c.access_dict.values():\n",
    "      relation = isl.map(\n",
    "          f\"{{ {stmt_name}[{','.join(domain_dims)}] -> {bf.name}[{','.join(map(lambda x: x.name, indices))}]}}\")\n",
    "      domain = domain.intersect(relation.intersect_range(bf.domain).domain())\n",
    "      for i, idx in enumerate(indices):\n",
    "        if idx.name not in bounded_iters:\n",
    "          bounded_iters[idx.name] = replace(idx, lower_bound=bf.domain.dim_min_val(\n",
    "              i).num_si(), upper_bound=bf.domain.dim_max_val(i).num_si() + 1)\n",
    "      access = Access(replace(bf, dims=tuple(\n",
    "          [bounded_iters[idx.name] for idx in indices])), relation)\n",
    "      accesses.append(access)\n",
    "      for i, idx in enumerate(indices):\n",
    "        j = iters.index(idx)\n",
    "        binding = dim_bindings.get(j, [])\n",
    "        binding.append((len(accesses) - 1, i))\n",
    "        dim_bindings[j] = binding\n",
    "\n",
    "    comp = Computation(c.op, domain, domain.identity(), accesses,\n",
    "                       tuple([bounded_iters[v.name] for v in iters]), dim_bindings=dim_bindings)\n",
    "    self.computations.append(comp)\n",
    "\n",
    "  def analysis_op(self, op: ir.Operation):\n",
    "    if isinstance(op, AssignOp):\n",
    "      self.analysis_stmt(op)\n",
    "\n",
    "  def apply(self, ctx, op: builtin.ModuleOp) -> dict[str, Computation]:\n",
    "    for sub in op.walk():\n",
    "      self.analysis_op(sub)\n",
    "    return self.computations\n",
    "\n",
    "\n",
    "s0 = PolyhedronExtractPass().apply(passes.Context(allow_unregistered=True), matmul.module)[0]\n",
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41a2f4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[k' = k, mo, mi = m - 8mo, n' = n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -7 + m <= 8mo <= m }\"), accesses=[Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))], iter_vars=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), mesh=None, iter_kinds={}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(self: Computation, parent_var: str, outer_var: str, inner_var: str, factor: int) -> 'Computation':\n",
    "  \"\"\" factor = extend(inner_var) \"\"\"\n",
    "  dim_index = self.iter_vars.index(parent_var)\n",
    "  assert dim_index != -1\n",
    "  new_iter_vars = tuple([*self.iter_vars[:dim_index],\n",
    "                         outer_var, inner_var, *self.iter_vars[dim_index + 1:]])\n",
    "\n",
    "  constraints_str = f' {outer_var} = {parent_var} // {factor} and {inner_var} = {parent_var} - {outer_var} * {factor}'\n",
    "  mapping_str = f\"{{ {self.name()}[{str.join(',', map(str, self.iter_vars))}] -> {self.name()}[{str.join(',', map(str, new_iter_vars))}]: {constraints_str} }}\"\n",
    "  split_map = isl.map(mapping_str)\n",
    "\n",
    "  new_schedule = self.schedule.apply_range(split_map)\n",
    "  new_schedule = reduce(lambda sche, p: sche.set_dim_name(\n",
    "      isl.dim_type.OUT, p[0], str(p[1])), enumerate(new_iter_vars), new_schedule)\n",
    "  return replace(self, iter_vars=tuple(new_iter_vars), schedule=new_schedule)\n",
    "\n",
    "\n",
    "Computation.split = split\n",
    "\n",
    "k, m, n = s0.iter_vars\n",
    "mo = IterVar.symbol('mo')\n",
    "mi = IterVar.symbol('mi')\n",
    "\n",
    "s0_splited = s0.split(m, mo, mi, 8)\n",
    "s0_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80ee18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[k' = k, mo, mi = m - 8mo, no, ni = n - 128no] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -7 + m <= 8mo <= m and -127 + n <= 128no <= n }\"), accesses=[Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))], iter_vars=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1)), mesh=None, iter_kinds={}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dim_bounds(self: Computation, dim_var: str) -> tuple[isl.val, isl.val]:\n",
    "  domain = self.schedule.intersect_domain(self.domain).range()\n",
    "  return (domain.dim_min_val(self.iter_vars.index(dim_var)), domain.dim_max_val(self.iter_vars.index(dim_var)))\n",
    "\n",
    "\n",
    "Computation.dim_bounds = dim_bounds\n",
    "\n",
    "\n",
    "def divide(self: Computation, parent_var: str, outer_var: str, inner_var: str, factor: int) -> 'Computation':\n",
    "  (min_val, max_val) = self.dim_bounds(parent_var)\n",
    "  return split(self, parent_var, outer_var, inner_var, (max_val.num_si() - min_val.num_si() + 1) // factor)\n",
    "\n",
    "\n",
    "Computation.divide = divide\n",
    "\n",
    "no = IterVar.symbol('no')\n",
    "ni = IterVar.symbol('ni')\n",
    "s0_divided = s0_splited.divide(n, no, ni, 8)\n",
    "s0_divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d9fb642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, mi = m - 8mo, ni = n - 128no, k' = k] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -7 + m <= 8mo <= m and -127 + n <= 128no <= n }\"), accesses=[Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))], iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), mesh=None, iter_kinds={}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reorder(self: Computation, *vars: List[int]):\n",
    "  assert len(set(vars)) == len(vars)\n",
    "  g = iter(vars)\n",
    "  new_iter_vars = [var if var not in vars else next(g) for var in self.iter_vars]\n",
    "  name = self.domain.get_tuple_name()\n",
    "  transform = isl.map(\n",
    "      f\"{{ {name}[{','.join(map(str,self.iter_vars))}] -> {name}[{','.join(map(str,new_iter_vars))}] }}\")\n",
    "  new_schedule = self.schedule.apply_range(transform)\n",
    "  new_schedule = reduce(lambda sche, p: sche.set_dim_name(\n",
    "      isl.dim_type.OUT, p[0], str(p[1])), enumerate(new_iter_vars), new_schedule)\n",
    "  return replace(self, iter_vars=tuple(new_iter_vars), schedule=new_schedule)\n",
    "\n",
    "\n",
    "Computation.reorder = reorder\n",
    "\n",
    "s0_ordered = s0_divided.reorder(mo, no, mi, ni, k)\n",
    "s0_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a27b70c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, ko, mi = m - 64mo, ni = n - 128no, ki = k - 256ko] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -63 + m <= 64mo <= m and -127 + n <= 128no <= n and -255 + k <= 256ko <= k }\"), accesses=[Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=None, usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))], iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ko', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='ki', lower_bound=None, upper_bound=None, step=1)), mesh=Mesh(dims=(IterVar(name='x', lower_bound=0, upper_bound=8, step=1), IterVar(name='y', lower_bound=0, upper_bound=8, step=1))), iter_kinds={IterVar(name='mo', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='no', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distribute(self: Computation, parent_vars: List[IterVar], outer_vars: List[IterVar], inner_vars: List[IterVar], mesh: Mesh):\n",
    "  assert self.mesh is None\n",
    "  assert len(parent_vars) == len(outer_vars) == len(inner_vars)\n",
    "  assert all(map(lambda k: k != IterKind.Distributed, self.iter_kinds.values()))\n",
    "  for i, mesh_dim in enumerate(mesh.dims):\n",
    "    self = self.divide(parent_vars[i], outer_vars[i], inner_vars[i], mesh_dim.extent)\n",
    "  self = self.reorder(chain(outer_vars, inner_vars))\n",
    "  new_iter_kinds = self.iter_kinds.copy()\n",
    "  new_iter_kinds.update({v: IterKind.Distributed for v in outer_vars})\n",
    "  return replace(self, iter_kinds=new_iter_kinds, mesh=mesh)\n",
    "\n",
    "\n",
    "Computation.distribute = distribute\n",
    "\n",
    "x, y = IterVar.range('x', 8), IterVar.range('y', 8)\n",
    "mo, no, mi, ni = IterVar.symbol('mo no mi ni')\n",
    "ko, ki = IterVar.symbol('ko ki')\n",
    "s0_distributed = s0.distribute([m, n], [mo, no], [mi, ni], Mesh((x, y))). \\\n",
    "    divide(k, ko, ki, x.extent). \\\n",
    "    reorder(mo, no, ko, mi, ni, ki)\n",
    "s0_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d5ba57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, ko, mi = m - 64mo, ni = n - 128no, ki = k - 256ko] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -63 + m <= 64mo <= m and -127 + n <= 128no <= n and -255 + k <= 256ko <= k }\"), accesses=(Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ C[m, n] -> C[x, y, m - 64x, local_n = n - 128y] : 0 <= m <= 511 and 0 <= n <= 1023 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))), iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ko', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='ki', lower_bound=None, upper_bound=None, step=1)), mesh=Mesh(dims=(IterVar(name='x', lower_bound=0, upper_bound=8, step=1), IterVar(name='y', lower_bound=0, upper_bound=8, step=1))), iter_kinds={IterVar(name='mo', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='no', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class ShardAxis:\n",
    "  lhs: IterVar\n",
    "  rhs: Tuple[IterVar]\n",
    "\n",
    "\n",
    "def var_shard_op(self: IterVar, other: IterVar | List[IterVar]):\n",
    "  return ShardAxis(self, (other,) if isinstance(other, IterVar) else tuple(other))\n",
    "\n",
    "\n",
    "IterVar.__matmul__ = var_shard_op\n",
    "\n",
    "\n",
    "def shard(self: Computation, buffer_name: str, *shard_axes: ShardAxis):\n",
    "  assert all(map(lambda v: v in self.mesh.dims, chain.from_iterable(\n",
    "      shard_axis.rhs for shard_axis in shard_axes))), \"rhs must be in mesh dimensions\"\n",
    "  access = next(filter(lambda acc: acc.buffer.name == buffer_name, self.accesses))\n",
    "  assert access\n",
    "  assert all([shard_axis.lhs in access.buffer.dims for shard_axis in shard_axes]\n",
    "             ), \"lhs must be in buffer dimensions\"\n",
    "  buffer = access.buffer\n",
    "  assert buffer.sharding == None, \"Buffer must not be sharded\"\n",
    "\n",
    "  def bound_fn(v): return f'{v.lower_bound} <= {v} < {v.upper_bound}'\n",
    "  sharding_map = isl.map(\n",
    "      f\"{{ {buffer.name}[{','.join(map(str,buffer.dims))}] -> {buffer.name}[{','.join(map(str,self.mesh.dims))},{','.join(map(str,buffer.dims))}] : {' and '.join(map(bound_fn, chain(self.mesh.dims, buffer.dims)))} }}\")\n",
    "\n",
    "  local = 'local_'\n",
    "  space_dims = list(map(str,  chain(self.mesh.dims, buffer.dims)))\n",
    "  for shard_axis in shard_axes:\n",
    "    lhs = shard_axis.lhs\n",
    "    dim_extent = lhs.extent\n",
    "    for rhs in shard_axis.rhs:\n",
    "      match rhs:\n",
    "        # todo support expr.\n",
    "        case IterVar():\n",
    "          assert (dim_extent % rhs.extent) == 0, \"Dimension can't divide evenly\"\n",
    "          dim_extent = tile = dim_extent // rhs.extent\n",
    "          constraints = f'{rhs} = {lhs} // {tile} and {local + str(lhs)} = {lhs} - {rhs} * {tile}'\n",
    "          lhs_space = f\"{buffer.name}[{','.join(space_dims)}]\"\n",
    "          rhs_space = f\"{buffer.name}[{','.join([local + str(lhs) if dim == str(lhs) else dim for dim in space_dims])}]\"\n",
    "          sharding_map = sharding_map.apply_range(\n",
    "              isl.map(f'{{ {lhs_space} -> {rhs_space} : {constraints} }}'))\n",
    "        case _:\n",
    "          raise NotImplementedError()\n",
    "  sharding_map = reduce(lambda sche, p: sche.set_dim_name(\n",
    "      isl.dim_type.OUT, p[0], str(p[1])), enumerate(self.mesh.dims), sharding_map)\n",
    "  n_access = replace(access, buffer=replace(buffer, sharding=sharding_map))\n",
    "  return replace(self, accesses=tuple([n_access if o_access.buffer.name == buffer.name else o_access for o_access in self.accesses]))\n",
    "\n",
    "\n",
    "Computation.shard = shard\n",
    "\n",
    "s0_sharded = s0_distributed.shard('A', m @ x, k @ y). \\\n",
    "    shard('B', k @ x, n @ y). \\\n",
    "    shard('C', m @ x, n @ y)\n",
    "s0_sharded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38aa134e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op='arith.mulf', domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, ko, mi = m - 64mo, ni = n - 128no, ki = k - 256ko] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -63 + m <= 64mo <= m and -127 + n <= 128no <= n and -255 + k <= 256ko <= k }\"), accesses=(Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ C[m, n] -> C[x, y, m - 64x, local_n = n - 128y] : 0 <= m <= 511 and 0 <= n <= 1023 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))), iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ko', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='ki', lower_bound=None, upper_bound=None, step=1)), mesh=Mesh(dims=(IterVar(name='x', lower_bound=0, upper_bound=8, step=1), IterVar(name='y', lower_bound=0, upper_bound=8, step=1))), iter_kinds={IterVar(name='mo', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='no', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>}, dim_bindings={1: [(0, 0), (1, 0)], 2: [(0, 1), (2, 1)], 0: [(1, 1), (2, 0)]}, transfers={IterVar(name='ko', lower_bound=None, upper_bound=None, step=1): (Transfer(access=Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")), Transfer(access=Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")))})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def communicate(self: Computation, buffer_name: str, var: IterVar, rotate_factors: List[IterVar] = []):\n",
    "  access = next(filter(lambda acc: acc.buffer.name == buffer_name, self.accesses))\n",
    "  assert access.buffer.sharding\n",
    "  assert IterKind.Serial == self.iter_kinds.get(var, IterKind.Serial)\n",
    "  new_transfers = self.transfers.copy()\n",
    "  transed = new_transfers.get(var, ())\n",
    "  assert access not in transed\n",
    "\n",
    "  i = self.iter_vars.index(var)\n",
    "\n",
    "  access_schedule = None\n",
    "  if len(rotate_factors) > 0:\n",
    "    name = self.name()\n",
    "    extent = self.schedule.range().dim_max_val(i).num_si() + 1\n",
    "    nvar = str(var) + '_r'\n",
    "    niter_vars = tuple([*self.iter_vars[:i], nvar, *self.iter_vars[i + 1:]])\n",
    "    constraint = f\"{nvar} = ({' + '.join(map(str, rotate_factors))} + {var}) mod {extent}\"\n",
    "    access_schedule = isl.map(\n",
    "        f\"{{ {name}[{','.join(map(str, self.iter_vars))}] -> {name}[{','.join(map(str, niter_vars))}] : {constraint} }}\")\n",
    "  else:\n",
    "    access_schedule = self.schedule.range().identity()\n",
    "\n",
    "  # check validity\n",
    "  schedule_to_sharding = self.schedule.apply_range(access_schedule).apply_domain(\n",
    "      access.relation).apply_domain(access.buffer.sharding).reverse()  # schedule -> buffer\n",
    "  dist_to_shard = schedule_to_sharding.project_out(isl.dim_type.IN, i + 1, len(self.iter_vars) - i - 1). \\\n",
    "      project_out(isl.dim_type.OUT, len(self.mesh.dims),\n",
    "                  access.buffer.sharding.dim(isl.dim_type.OUT) - len(self.mesh.dims))\n",
    "  assert dist_to_shard.is_single_valued(), \"transfer can't read/write data cross multi nodes.\"\n",
    "\n",
    "  new_transfers[var] = (Transfer(access, access_schedule), *transed)\n",
    "  return replace(self, transfers=new_transfers)\n",
    "\n",
    "\n",
    "Computation.communicate = communicate\n",
    "\n",
    "s0_communicated = s0_sharded.communicate('A', ko). \\\n",
    "    communicate('B', ko)\n",
    "\n",
    "s0_communicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24465d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpKind(IntEnum):\n",
    "  # call(Assign, dest, src)\n",
    "  Assign = 0\n",
    "  # call(Access, buffer, *(int | slice))\n",
    "  Access = 1\n",
    "  # call(Trans, commPattern, sendbuf, dest, recvbuf, source)\n",
    "  Trans = 2\n",
    "  # call(Alloc, name, *dims)\n",
    "  Alloc = 3\n",
    "  # call(Rank, *ids)\n",
    "  Rank = 4\n",
    "  # call(CommSendrecv)\n",
    "  CommSendrecv = 5\n",
    "  # call(CommBroadcast, *commGroups)\n",
    "  CommBroadcast = 6\n",
    "  # call(CommShift, *commGroups, direction)\n",
    "  CommShift = 7\n",
    "  # call(AssertEqual, a, b)\n",
    "  AssertEqual = 8\n",
    "  # call(AugAssign, dest, src)\n",
    "  AugAssign = 9\n",
    "\n",
    "  # call(Slice, begin, end)\n",
    "  Slice = 128\n",
    "  Add = 129\n",
    "  Mul = 130\n",
    "  MatMul = 131\n",
    "\n",
    "\n",
    "def call_from(build: isl.ast_build, op: OpKind, *args: List[isl.ast_expr | str]):\n",
    "  assert isinstance(op, OpKind)\n",
    "  l = isl.ast_expr_list(len(args))\n",
    "  for i in range(len(args)):\n",
    "    match args[i]:\n",
    "      case isl.ast_expr():\n",
    "        l = l.add(args[i])\n",
    "      case int() | isl.val():\n",
    "        l = l.add(isl.ast_expr.from_val(args[i]))\n",
    "      case str():\n",
    "        l = l.add(isl.ast_expr.from_id(args[i]))\n",
    "      case isl.pw_aff():\n",
    "        l = l.add(build.expr_from(args[i]))\n",
    "      case _:\n",
    "        raise ValueError(f\"Unsupported argument type: {type(args[i])}\")\n",
    "  return isl.ast_expr.call(isl.ast_expr.from_id(op.name), l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f18a9",
   "metadata": {},
   "source": [
    "开始生成计算代码，由于transfer调度引入了新的临时buffer/数据通信，以及tensorize带来的改变。\n",
    "\n",
    "为正确的构造他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c021b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransferScheduleInfo(dim=2, access_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> B[256ko + ki, 128no + ni] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), access_shard_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> B[x, y, 256ko + ki - 256x, local_n = 128no + ni - 128y] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 and 0 <= x <= 7 and -255 + 256ko + ki <= 256x <= 256ko + ki and 0 <= y <= 7 and -127 + 128no + ni <= 128y <= 128no + ni }\"), access_schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), box_hull=isl.fixed_box(\"\"\"{ offset: \"{ s0[mo, no, ko, mi, ni, ki] -> B[(256ko), (128no)] }\", size: \"{ B[256, 128] }\" }\"\"\"), alloc_schedule=isl.map(\"{ AllocTransB[mo, no, ko] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 0, mi = 0, 0, ni = 0, 0, ki = 0] : 0 <= mo <= 7 and 0 <= no <= 7 and 0 <= ko <= 7 }\"), trans_schedule=isl.map(\"{ TransB[mo, no, ko, mi = 0, ni, ki] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 1, mi' = 0, 0, ni' = ni, 0, ki' = ki] : 0 <= mo <= 7 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), redundancies=(3,))\n",
      "TransferScheduleInfo(dim=2, access_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> A[64mo + mi, 256ko + ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), access_shard_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> A[x, y, 64mo + mi - 64x, local_k = 256ko + ki - 256y] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 and 0 <= x <= 7 and -63 + 64mo + mi <= 64x <= 64mo + mi and 0 <= y <= 7 and -255 + 256ko + ki <= 256y <= 256ko + ki }\"), access_schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), box_hull=isl.fixed_box(\"\"\"{ offset: \"{ s0[mo, no, ko, mi, ni, ki] -> A[(64mo), (256ko)] }\", size: \"{ A[64, 256] }\" }\"\"\"), alloc_schedule=isl.map(\"{ AllocTransA[mo, no, ko] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 0, mi = 0, 0, ni = 0, 0, ki = 0] : 0 <= mo <= 7 and 0 <= no <= 7 and 0 <= ko <= 7 }\"), trans_schedule=isl.map(\"{ TransA[mo, no, ko, mi, ni = 0, ki] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 1, mi' = mi, 0, ni' = 0, 0, ki' = ki] : 0 <= no <= 7 and mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), redundancies=(4,))\n"
     ]
    }
   ],
   "source": [
    "def get_kelly_map(self: Computation, *tps: Tuple[int, int]):\n",
    "  ndim = self.schedule.dim(isl.dim_type.OUT)\n",
    "  sche = self.schedule.range().identity()\n",
    "  d = {p[0] + 1: p[1] for p in tps}\n",
    "  for i in range(ndim):\n",
    "    sche = sche.insert_dims(isl.dim_type.OUT, i * 2,\n",
    "                            1).fix_val(isl.dim_type.OUT, i * 2, d.get(i, 0))\n",
    "  return sche.set_tuple_name(isl.dim_type.OUT, sche.get_tuple_name(isl.dim_type.IN))\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class TransferScheduleInfo:\n",
    "  dim: int\n",
    "  access_map: isl.map\n",
    "  access_shard_map: isl.map  # schedule -> buffer\n",
    "  access_schedule: isl.map  # schedule -> new schedule\n",
    "  box_hull: isl.fixed_box\n",
    "  alloc_schedule: isl.map\n",
    "  trans_schedule: isl.map\n",
    "  redundancies: Tuple[int, ...]\n",
    "\n",
    "  @property\n",
    "  def alloc_name(self):\n",
    "    return self.alloc_schedule.tuple_name(isl.dim_type.IN)\n",
    "\n",
    "  @property\n",
    "  def trans_name(self):\n",
    "    return self.trans_schedule.tuple_name(isl.dim_type.IN)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class ComputationScheduleInfo:\n",
    "  comp_schedule: isl.map\n",
    "  assign_kind: OpKind\n",
    "  op_kind: OpKind\n",
    "\n",
    "  @property\n",
    "  def comp_name(self):\n",
    "    return self.comp_schedule.tuple_name(isl.dim_type.IN)\n",
    "\n",
    "\n",
    "def get_transfer_schedule_info(self: Computation, transfer: Transfer, dim: int, order: int):\n",
    "  sched_domain = self.schedule.range()\n",
    "  ndim = sched_domain.n_dim()\n",
    "  access_map = self.schedule.apply_domain(transfer.access.relation).reverse()\n",
    "  access_shard_map = access_map.apply_range(transfer.access.buffer.sharding)\n",
    "  box_hull = access_map. \\\n",
    "      eliminate(isl.dim_type.IN, dim + 1, ndim - dim - 1). \\\n",
    "      range_simple_fixed_box_hull()\n",
    "\n",
    "  trans_name = OpKind.Trans.name + transfer.access.buffer.name\n",
    "  alloc_name = OpKind.Alloc.name + trans_name\n",
    "  alloc_schedule = get_kelly_map(self, (dim, order)). \\\n",
    "      intersect_domain(sched_domain). \\\n",
    "      project_out(isl.dim_type.IN, dim + 1, ndim - (dim + 1)). \\\n",
    "      set_domain_tuple(alloc_name)\n",
    "  for drop_dim in range(dim + 1, ndim):\n",
    "    alloc_schedule = alloc_schedule.fix_si(isl.dim_type.OUT, drop_dim * 2 + 1, 0)\n",
    "  order += 1\n",
    "\n",
    "  trans_schedule = get_kelly_map(self, (dim, order)). \\\n",
    "      intersect_domain(sched_domain). \\\n",
    "      set_domain_tuple(trans_name)\n",
    "\n",
    "  # find dropped dimensions\n",
    "  redundancies = []\n",
    "  cons_free_map = access_map.drop_constraints_not_involving_dims(\n",
    "      isl.dim_type.OUT, 0, len(transfer.access.buffer.dims))\n",
    "  for i in range(dim, ndim):\n",
    "    if not cons_free_map.involves_dims(isl.dim_type.IN, i, 1):\n",
    "      trans_schedule = trans_schedule.fix_si(isl.dim_type.IN, i, 0)\n",
    "      redundancies.append(i)\n",
    "\n",
    "  order += 1\n",
    "\n",
    "  return TransferScheduleInfo(dim, access_map, access_shard_map, transfer.schedule,\n",
    "                              box_hull, alloc_schedule, trans_schedule,\n",
    "                              tuple(redundancies))\n",
    "\n",
    "\n",
    "s0_trans_info_0 = get_transfer_schedule_info(\n",
    "    s0_communicated, s0_communicated.transfers[ko][0], 2, 0)\n",
    "s0_trans_info_1 = get_transfer_schedule_info(\n",
    "    s0_communicated, s0_communicated.transfers[ko][1], 2, 0)\n",
    "print(s0_trans_info_0)\n",
    "print(s0_trans_info_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757b75c",
   "metadata": {},
   "source": [
    "由于我们的backend是MPI，没办法直接根据rank id进行数据读写，只能依赖它提供的通信原语。比如当多个rank需要从同一个rank进行取数时， 需要使用broadcast原语才可以正常通信，如果使用sendrecv原语则需要指定rank多次调用。 因此针对通信调度还需要检测其通信模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb5951bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast(axes=(0,))\n",
      "Broadcast(axes=(1,))\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class CommPattern:\n",
    "  def build_call(self, build):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class SendRecv(CommPattern):\n",
    "  def build_call(self, build):\n",
    "    return call_from(build, OpKind.CommSendrecv)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class Broadcast(CommPattern):\n",
    "  axes: tuple[int, ...]\n",
    "\n",
    "  def build_call(self, build):\n",
    "    return call_from(build, OpKind.CommBroadcast, *self.axes)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class Shift(CommPattern):\n",
    "  axes: tuple[int, ...]\n",
    "  direction: int\n",
    "\n",
    "  def build_call(self, build):\n",
    "    return call_from(build, OpKind.CommShift, *self.axes, self.direction)\n",
    "\n",
    "\n",
    "def detect_communication_pattern(self: Computation, info: TransferScheduleInfo):\n",
    "  domain_ndim = info.access_map.dim(isl.dim_type.IN)\n",
    "  shard_ndim = info.access_shard_map.dim(isl.dim_type.OUT)\n",
    "  mesh_ndim = len(self.mesh.dims)\n",
    "  access_shard_map = info.access_shard_map.apply_domain(info.access_schedule)\n",
    "  access_src_rank = access_shard_map. \\\n",
    "      project_out(isl.dim_type.OUT, mesh_ndim, shard_ndim - mesh_ndim).\\\n",
    "      project_out(isl.dim_type.IN, info.dim + 1, domain_ndim - info.dim - 1)\n",
    "  access_src_pma = access_src_rank.as_pw_multi_aff()\n",
    "  access_next_src_pma = isl.pw_multi_aff.identity_on_domain(\n",
    "      access_src_pma.domain_space()).add_constant(isl.multi_val(f\"{{ [{','.join(['0'] * info.dim + ['1'])}] }}\"))\n",
    "  src_rank_deltas = access_src_pma.pullback(access_next_src_pma).sub(access_src_pma).coalesce()\n",
    "  comm_patterns = []\n",
    "  for i in range(mesh_ndim):\n",
    "    pa = src_rank_deltas.at(i)\n",
    "    if pa.is_cst():\n",
    "      match pa.max_val().num_si():\n",
    "        case 0:  # not involved\n",
    "          comm_patterns.append(SendRecv())\n",
    "        case 1:  # identity with time.\n",
    "          if not access_src_rank.is_bijective():  # detect broadcast\n",
    "            unbounded = access_src_rank.drop_constraints_not_involving_dims(\n",
    "                isl.dim_type.OUT, 0, mesh_ndim)\n",
    "            if not unbounded.involves_dims(isl.dim_type.IN, i, 1):\n",
    "              comm_patterns.append(Broadcast((i,)))\n",
    "            else:\n",
    "              comm_patterns.append(SendRecv())\n",
    "          else:\n",
    "            comm_patterns.append(SendRecv())\n",
    "    else:\n",
    "      points = []\n",
    "      pa.as_map().range().foreach_point(lambda x: points.append(isl.set(x)))\n",
    "      points = reduce(lambda acc, x: acc.union(x), points, isl.set.empty(\n",
    "          isl.space.unit().add_dims(isl.dim_type.SET, 1)))\n",
    "      extent = self.schedule.range().dim_max_val(info.dim).num_si()\n",
    "      cw_set = isl.set(f'{{[1]; [-{extent}]}}')\n",
    "      ccw_set = isl.set(f'{{[-1]; [{extent}]}}')\n",
    "      if points.is_equal(cw_set):\n",
    "        comm_patterns.append(Shift((i,), 1))\n",
    "      elif points.is_equal(ccw_set):\n",
    "        comm_patterns.append(Shift((i,), -1))\n",
    "\n",
    "  special = sum([isinstance(p, (Broadcast, Shift)) for p in comm_patterns])\n",
    "  assert special <= 1\n",
    "  return SendRecv() if special == 0 else next(filter(lambda p: not isinstance(p, SendRecv), comm_patterns))\n",
    "\n",
    "\n",
    "print(detect_communication_pattern(s0_communicated, s0_trans_info_0))\n",
    "print(detect_communication_pattern(s0_communicated, s0_trans_info_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3a6589f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScheduleInfo(trans_infos=(TransferScheduleInfo(dim=2, access_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> B[256ko + ki, 128no + ni] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), access_shard_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> B[x, y, 256ko + ki - 256x, local_n = 128no + ni - 128y] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 and 0 <= x <= 7 and -255 + 256ko + ki <= 256x <= 256ko + ki and 0 <= y <= 7 and -127 + 128no + ni <= 128y <= 128no + ni }\"), access_schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), box_hull=isl.fixed_box(\"\"\"{ offset: \"{ s0[mo, no, ko, mi, ni, ki] -> B[(256ko), (128no)] }\", size: \"{ B[256, 128] }\" }\"\"\"), alloc_schedule=isl.map(\"{ AllocTransB[mo, no, ko] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 0, mi = 0, 0, ni = 0, 0, ki = 0] : 0 <= mo <= 7 and 0 <= no <= 7 and 0 <= ko <= 7 }\"), trans_schedule=isl.map(\"{ TransB[mo, no, ko, mi = 0, ni, ki] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 1, mi' = 0, 0, ni' = ni, 0, ki' = ki] : 0 <= mo <= 7 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), redundancies=(3,)), TransferScheduleInfo(dim=2, access_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> A[64mo + mi, 256ko + ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), access_shard_map=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> A[x, y, 64mo + mi - 64x, local_k = 256ko + ki - 256y] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 and 0 <= x <= 7 and -63 + 64mo + mi <= 64x <= 64mo + mi and 0 <= y <= 7 and -255 + 256ko + ki <= 256y <= 256ko + ki }\"), access_schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), box_hull=isl.fixed_box(\"\"\"{ offset: \"{ s0[mo, no, ko, mi, ni, ki] -> A[(64mo), (256ko)] }\", size: \"{ A[64, 256] }\" }\"\"\"), alloc_schedule=isl.map(\"{ AllocTransA[mo, no, ko] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 2, mi = 0, 0, ni = 0, 0, ki = 0] : 0 <= mo <= 7 and 0 <= no <= 7 and 0 <= ko <= 7 }\"), trans_schedule=isl.map(\"{ TransA[mo, no, ko, mi, ni = 0, ki] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 3, mi' = mi, 0, ni' = 0, 0, ki' = ki] : 0 <= no <= 7 and mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), redundancies=(4,))), comp_infos=(ComputationScheduleInfo(comp_schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[0, mo' = mo, 0, no' = no, 0, ko' = ko, 4, mi' = mi, 0, ni' = ni, 0, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\"), assign_kind=<OpKind.AugAssign: 9>, op_kind=<OpKind.Mul: 130>),))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class ScheduleInfo:\n",
    "  trans_infos: Tuple[TransferScheduleInfo]\n",
    "  comp_infos: Tuple[ComputationScheduleInfo]\n",
    "\n",
    "\n",
    "def get_schedule_info(self: Computation):\n",
    "  transfer_sche_infos = []\n",
    "  used_orders = []\n",
    "  for (var, accesses) in self.transfers.items():\n",
    "    dim = self.iter_vars.index(var)\n",
    "    order = 0\n",
    "    for access in accesses:\n",
    "      transfer_sche_infos.append(get_transfer_schedule_info(self, access, dim, order))\n",
    "      order += 2\n",
    "      used_orders.append((dim, order))\n",
    "  in_iters = set(itertools.flatten(\n",
    "      [acc.buffer.dims for acc in self.accesses if acc.buffer.usage is UsageKind.Input]))\n",
    "  out_iters = set(itertools.flatten(\n",
    "      [acc.buffer.dims for acc in self.accesses if acc.buffer.usage is UsageKind.Output]))\n",
    "\n",
    "  op_kind = None\n",
    "  match self.op:\n",
    "    case OpKind():\n",
    "      op_kind = self.op\n",
    "    case 'arith.mulf':\n",
    "      op_kind = OpKind.Mul\n",
    "    case _:\n",
    "      raise NotImplementedError()\n",
    "\n",
    "  comp_schedule = ComputationScheduleInfo(get_kelly_map(\n",
    "      self, *used_orders).intersect_domain(self.schedule.range()),\n",
    "      OpKind.AugAssign if len(in_iters) > len(out_iters) else OpKind.Assign,\n",
    "      op_kind)\n",
    "  return ScheduleInfo(tuple(transfer_sche_infos), (comp_schedule,))\n",
    "\n",
    "\n",
    "s0_schedule_info = get_schedule_info(s0_communicated)\n",
    "s0_schedule_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea79a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for (int x = 0; x <= 7; x += 1)\n",
      "  for (int y = 0; y <= 7; y += 1)\n",
      "    for (int ko = 0; ko <= 7; ko += 1) {\n",
      "      Alloc(TransB, 256, 128);\n",
      "      for (int ni = 0; ni <= 127; ni += 1)\n",
      "        for (int ki = 0; ki <= 255; ki += 1)\n",
      "          Trans(CommBroadcast(0), Access(B, Slice(ki, ki + 1), Slice(ni, ni + 1)), Rank(ko), Access(TransB, Slice(ki, ki + 1), Slice(ni, ni + 1)), Rank(x));\n",
      "      Alloc(TransA, 64, 256);\n",
      "      for (int mi = 0; mi <= 63; mi += 1)\n",
      "        for (int ki = 0; ki <= 255; ki += 1)\n",
      "          Trans(CommBroadcast(1), Access(A, Slice(mi, mi + 1), Slice(ki, ki + 1)), Rank(ko), Access(TransA, Slice(mi, mi + 1), Slice(ki, ki + 1)), Rank(y));\n",
      "      for (int mi = 0; mi <= 63; mi += 1)\n",
      "        for (int ni = 0; ni <= 127; ni += 1)\n",
      "          for (int ki = 0; ki <= 255; ki += 1)\n",
      "            AugAssign(Access(C, Slice(mi, mi + 1), Slice(ni, ni + 1)), Mul(Access(TransA, Slice(mi, mi + 1), Slice(ki, ki + 1)), Access(TransB, Slice(ki, ki + 1), Slice(ni, ni + 1))));\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lower_computation(self: Computation):\n",
    "  schedule_info = get_schedule_info(self)\n",
    "  # process tensorized\n",
    "  sched_domain = self.schedule.range()\n",
    "  sched_domain_min = self.schedule.range()\n",
    "  sched_domain_max = self.schedule.range()\n",
    "  tensorized_dims = []\n",
    "  for i, v in enumerate(self.iter_vars):\n",
    "    if self.iter_kinds.get(v) is IterKind.Tensorize:\n",
    "      sched_domain_min = sched_domain_min.fix_val(isl.dim_type.SET, i, sched_domain.dim_min_val(i))\n",
    "      sched_domain_max = sched_domain_max.fix_val(isl.dim_type.SET, i, sched_domain.dim_max_val(i))\n",
    "      tensorized_dims.append(i)\n",
    "\n",
    "  def fix_dims(sche: isl.map):\n",
    "    for d in tensorized_dims:\n",
    "      sche = sche.fix_si(isl.dim_type.OUT, (2 * d) + 1, 0)\n",
    "    return sche\n",
    "\n",
    "  def drop_dims(sche: isl.map | isl.aff, redundancies: Tuple[int] = ()):\n",
    "    dims = list(set([*redundancies, *tensorized_dims]))\n",
    "    dims.sort()\n",
    "    j = 0\n",
    "    for i in range(len(dims)):\n",
    "      if isinstance(sche, isl.map):\n",
    "        sche = sche.project_out(isl.dim_type.IN, dims[i] - j, 1)\n",
    "      elif isinstance(sche, isl.pw_aff):\n",
    "        sche = sche.drop_dims(isl.dim_type.IN, dims[i] - j, 1)\n",
    "      else:\n",
    "        raise NotImplementedError\n",
    "      j += 1\n",
    "    return sche\n",
    "\n",
    "  def get_box(map: isl.map) -> isl.multi_val:\n",
    "    min = drop_dims(map.intersect_domain(sched_domain_min))\n",
    "    max = drop_dims(map.intersect_domain(sched_domain_max))\n",
    "    diff = max.as_pw_multi_aff().sub(min.as_pw_multi_aff())\n",
    "    assert diff.is_cst()\n",
    "    return diff.max_multi_val()\n",
    "\n",
    "  full_sche_map = isl.union_map.empty()\n",
    "  for info in schedule_info.comp_infos:\n",
    "    full_sche_map = full_sche_map.union(fix_dims(info.comp_schedule))\n",
    "  for info in schedule_info.trans_infos:\n",
    "    full_sche_map = full_sche_map.union(fix_dims(info.alloc_schedule))\n",
    "    full_sche_map = full_sche_map.union(fix_dims(info.trans_schedule))\n",
    "  alloc_info_map = {info.alloc_name: info for info in schedule_info.trans_infos}\n",
    "  trans_info_map = {info.trans_name: info for info in schedule_info.trans_infos}\n",
    "  comp_info_map = {info.comp_name: info for info in schedule_info.comp_infos}\n",
    "\n",
    "  def at_each_domain(node: isl.ast_node_user, build: isl.ast_build) -> isl.ast_node:\n",
    "    origin_expr = node.expr()\n",
    "    if not isinstance(origin_expr, isl.ast_expr_op_call):\n",
    "      return node\n",
    "\n",
    "    call_id: isl.ast_expr_id = origin_expr.op_arg(0)\n",
    "    call_id_name = call_id.id().name()\n",
    "    # alloc\n",
    "    if call_id_name in alloc_info_map:\n",
    "      info = alloc_info_map[call_id_name]\n",
    "      box_shape = info.box_hull.get_size()\n",
    "      rank = box_shape.size()\n",
    "      alloc = call_from(build, OpKind.Alloc, info.trans_name,\n",
    "                        *[box_shape.at(i) for i in range(rank)])\n",
    "      return isl.ast_node_user(alloc)\n",
    "\n",
    "    # trans\n",
    "    if call_id_name in trans_info_map:\n",
    "      info = trans_info_map[call_id_name]\n",
    "      pattern = detect_communication_pattern(self, info)\n",
    "      select_ranks = list(range(len(self.mesh.dims)))\n",
    "      match pattern:\n",
    "        case Broadcast():\n",
    "          select_ranks = list(filter(lambda i: i in pattern.axes, select_ranks))\n",
    "\n",
    "      def drop_dims2(x): return drop_dims(x, info.redundancies)\n",
    "\n",
    "      src_shard_pma = info.access_shard_map.as_pw_multi_aff()\n",
    "      access_sche_pma = info.access_schedule.as_pw_multi_aff()\n",
    "      src_shard_pma = src_shard_pma.pullback(access_sche_pma)\n",
    "      src_rank = call_from(build, OpKind.Rank, *[drop_dims2(src_shard_pma.at(i))\n",
    "                           for i in select_ranks])\n",
    "\n",
    "      dest_rank_pma = info.access_map.domain().identity().as_pw_multi_aff()\n",
    "      dest_rank_pma = dest_rank_pma.pullback(access_sche_pma)\n",
    "      dest_rank = call_from(build, OpKind.Rank, *\n",
    "                            [drop_dims2(dest_rank_pma.at(i)) for i in select_ranks])\n",
    "\n",
    "      src_tensor_box = get_box(info.access_shard_map)\n",
    "      src_slice = call_from(build, OpKind.Access, info.access_shard_map.tuple_name(isl.dim_type.OUT),\n",
    "                            *[call_from(build, OpKind.Slice, drop_dims2(src_shard_pma.at(i)),\n",
    "                                        drop_dims2(src_shard_pma.at(i)).add_constant(src_tensor_box.at(i)).add_constant(1))\n",
    "                              for i in range(len(self.mesh.dims), src_shard_pma.dim(isl.dim_type.OUT))])\n",
    "\n",
    "      dest_start_pma = info.box_hull.get_offset().as_pw_multi_aff()\n",
    "      dest_origin_pma = info.access_map.as_pw_multi_aff()\n",
    "      dest_tensor_box = get_box(info.access_map)\n",
    "      dest_pma = dest_origin_pma.sub(dest_start_pma)\n",
    "      dest_pma = dest_pma.pullback(access_sche_pma)\n",
    "      dest_slice = call_from(build, OpKind.Access, info.trans_name,\n",
    "                             *[call_from(build, OpKind.Slice, drop_dims2(dest_pma.at(i)),\n",
    "                                         drop_dims2(dest_pma.at(i)).add_constant(dest_tensor_box.at(i)).add_constant(1))\n",
    "                               for i in range(dest_pma.size())])\n",
    "\n",
    "      trans = call_from(build, OpKind.Trans, pattern.build_call(build),\n",
    "                        src_slice, src_rank, dest_slice, dest_rank)\n",
    "      return isl.ast_node_user(trans)\n",
    "    if call_id_name in comp_info_map:\n",
    "      info = comp_info_map[call_id_name]\n",
    "      access_exprs = []\n",
    "      for access in self.accesses:\n",
    "        trans_name = OpKind.Trans.name + access.buffer.name\n",
    "        if trans_name in trans_info_map:\n",
    "          trans_info = trans_info_map[trans_name]\n",
    "          access_sche_pma = trans_info.access_schedule.as_pw_multi_aff()\n",
    "          dest_start_pma = trans_info.box_hull.get_offset().as_pw_multi_aff()\n",
    "          dest_origin_pma = trans_info.access_map.as_pw_multi_aff()\n",
    "          dest_tensor_box = get_box(trans_info.access_map)\n",
    "          dest_pma = dest_origin_pma.sub(dest_start_pma)\n",
    "          dest_pma = dest_pma.pullback(access_sche_pma)\n",
    "          access_exprs.append(call_from(build, OpKind.Access, trans_name, *\n",
    "                              [call_from(build, OpKind.Slice, drop_dims(dest_pma.at(i)), drop_dims(dest_pma.at(i)).add_constant(dest_tensor_box.at(i)).add_constant(1))\n",
    "                               for i in range(dest_pma.size())]))\n",
    "        else:\n",
    "          access_shard_map = self.schedule.reverse().apply_range(\n",
    "              access.relation).apply_range(access.buffer.sharding)\n",
    "          dest_shard_pma = access_shard_map.as_pw_multi_aff()\n",
    "          dest_tensor_box = get_box(access_shard_map)\n",
    "          dest_slice = call_from(build, OpKind.Access, access.buffer.name, *[\n",
    "              call_from(build, OpKind.Slice, drop_dims(dest_shard_pma.at(i)),\n",
    "                        drop_dims(dest_shard_pma.at(i)).add_constant(dest_tensor_box.at(i)).add_constant(1))\n",
    "              for i in range(len(self.mesh.dims), dest_shard_pma.dim(isl.dim_type.OUT))])\n",
    "          access_exprs.append(dest_slice)\n",
    "      call_expr = call_from(build, info.assign_kind,\n",
    "                            access_exprs[0],\n",
    "                            call_from(build, info.op_kind, access_exprs[1], access_exprs[2]))\n",
    "      return isl.ast_node_user(call_expr)\n",
    "    return node\n",
    "\n",
    "  builtin_iters = list(map(str, self.mesh.dims))\n",
    "\n",
    "  def at_each_for(node: isl.ast_node_for, build: isl.ast_build) -> isl.ast_node:\n",
    "    it = node.get_iterator()\n",
    "    if isinstance(it, isl.ast_expr_id) and it.id().name() in builtin_iters:\n",
    "      node = node.set_annotation(IterKind.Distributed.name)\n",
    "    return node\n",
    "\n",
    "  ast_build = isl.ast_build()\n",
    "  ast_build = ast_build.set_at_each_domain(at_each_domain)\n",
    "  ast_build = ast_build.set_after_each_for(at_each_for)\n",
    "  iter_ids = []\n",
    "  ndim = len(self.iter_vars)\n",
    "  comp_schedule = schedule_info.comp_infos[0].comp_schedule\n",
    "  iter_kinds = {k.name: v for (k, v) in self.iter_kinds.items()}\n",
    "  for i in range(ndim):\n",
    "    iter_ids.append(f'c{i}')\n",
    "    name = comp_schedule.dim_name(isl.dim_type.OUT, 2 * i + 1)\n",
    "    if iter_kinds.get(name) is IterKind.Distributed:\n",
    "      name = str(self.mesh.dims[i])\n",
    "    iter_ids.append(name)\n",
    "  ast_build = ast_build.set_iterators('(' + ','.join(iter_ids) + ')')\n",
    "  ast_node = ast_build.node_from_schedule_map(full_sche_map)\n",
    "  return ast_node\n",
    "\n",
    "\n",
    "with open('tmp/1.py', 'w') as f:\n",
    "  ast_computation = lower_computation(s0_communicated)\n",
    "  printer = isl.printer.to_file(f)\n",
    "  print_options = isl.ast_print_options.alloc()\n",
    "  printer.set_output_format(isl.format.C)\n",
    "  printer = ast_computation.print(printer, print_options)\n",
    "  printer.flush()\n",
    "\n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793ba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op=<OpKind.MatMul: 131>, domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, ko, mi = m - 64mo, ni = n - 128no, ki = k - 256ko] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -63 + m <= 64mo <= m and -127 + n <= 128no <= n and -255 + k <= 256ko <= k }\"), accesses=(Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ C[m, n] -> C[x, y, m - 64x, local_n = n - 128y] : 0 <= m <= 511 and 0 <= n <= 1023 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))), iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ko', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='ki', lower_bound=None, upper_bound=None, step=1)), mesh=Mesh(dims=(IterVar(name='x', lower_bound=0, upper_bound=8, step=1), IterVar(name='y', lower_bound=0, upper_bound=8, step=1))), iter_kinds={IterVar(name='mo', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='no', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='mi', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>, IterVar(name='ni', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>, IterVar(name='ki', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>}, transfers={IterVar(name='ko', lower_bound=None, upper_bound=None, step=1): (Transfer(access=Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")), Transfer(access=Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")))})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tensorize(self: Computation, vars: List[IterVar], new_op: OpKind = None):\n",
    "  new_kinds = self.iter_kinds.copy()\n",
    "  for var in vars:\n",
    "    assert new_kinds.get(var, None) in (None, IterKind.Serial)\n",
    "    new_kinds[var] = IterKind.Tensorize\n",
    "  if new_op:\n",
    "    self = replace(self, op=new_op)\n",
    "  return replace(self, iter_kinds=new_kinds)\n",
    "\n",
    "\n",
    "Computation.tensorize = tensorize\n",
    "\n",
    "s0_tensorized = s0_communicated.tensorize([mi, ni, ki], OpKind.MatMul)\n",
    "s0_tensorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for (int x = 0; x <= 7; x += 1)\n",
      "  for (int y = 0; y <= 7; y += 1)\n",
      "    for (int ko = 0; ko <= 7; ko += 1) {\n",
      "      Alloc(TransB, 256, 128);\n",
      "      Trans(CommBroadcast(0), Access(B, Slice(0, 256), Slice(0, 128)), Rank(ko), Access(TransB, Slice(0, 256), Slice(0, 128)), Rank(x));\n",
      "      Alloc(TransA, 64, 256);\n",
      "      Trans(CommBroadcast(1), Access(A, Slice(0, 64), Slice(0, 256)), Rank(ko), Access(TransA, Slice(0, 64), Slice(0, 256)), Rank(y));\n",
      "      AugAssign(Access(C, Slice(0, 64), Slice(0, 128)), MatMul(Access(TransA, Slice(0, 64), Slice(0, 256)), Access(TransB, Slice(0, 256), Slice(0, 128))));\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/1.py', 'w') as f:\n",
    "  ast_computation = lower_computation(s0_tensorized)\n",
    "  printer = isl.printer.to_file(f)\n",
    "  print_options = isl.ast_print_options.alloc()\n",
    "  printer.set_output_format(isl.format.C)\n",
    "  printer = ast_computation.print(printer, print_options)\n",
    "  printer.flush()\n",
    "\n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a31144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alloc(C, 64, 128);\n",
      "for (int x = 0; x <= 7; x += 1)\n",
      "  for (int y = 0; y <= 7; y += 1)\n",
      "    for (int m = 0; m <= 63; m += 1)\n",
      "      for (int n = 0; n <= 127; n += 1)\n",
      "        AssertEqual(Access(C, m, n), Access(GlobalC, 64 * x + m, 128 * y + n));\n",
      "Alloc(A, 64, 256);\n",
      "for (int x = 0; x <= 7; x += 1)\n",
      "  for (int y = 0; y <= 7; y += 1)\n",
      "    for (int m = 0; m <= 63; m += 1)\n",
      "      for (int k = 0; k <= 255; k += 1)\n",
      "        Assign(Access(A, m, k), Access(GlobalA, 64 * x + m, 256 * y + k));\n",
      "Alloc(B, 256, 128);\n",
      "for (int x = 0; x <= 7; x += 1)\n",
      "  for (int y = 0; y <= 7; y += 1)\n",
      "    for (int k = 0; k <= 255; k += 1)\n",
      "      for (int n = 0; n <= 127; n += 1)\n",
      "        Assign(Access(B, k, n), Access(GlobalB, 256 * x + k, 128 * y + n));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lower_shard(self: Computation, access: Access) -> Tuple[isl.ast_build, isl.ast_build]:\n",
    "  sharding = access.buffer.sharding\n",
    "  assert sharding\n",
    "  builtin_iters = list(map(str, self.mesh.dims))\n",
    "\n",
    "  access_global_pma = sharding.reverse().set_tuple_id(\n",
    "      isl.dim_type.OUT, 'Global' + access.buffer.name).as_pw_multi_aff()\n",
    "  access_global_domain = sharding.reverse().domain()\n",
    "  buffer_local_shape = access_global_domain.project_out(\n",
    "      isl.dim_type.SET, 0, len(self.mesh.dims)).simple_fixed_box_hull().size()\n",
    "  access_local_map = sharding.reverse().domain().identity().project_out(\n",
    "      isl.dim_type.OUT, 0, len(self.mesh.dims)).set_range_tuple(access.buffer.name)\n",
    "  access_local_pma = access_local_map.as_pw_multi_aff()\n",
    "\n",
    "  ast_build = isl.ast_build()\n",
    "\n",
    "  def at_each_domain(node: isl.ast_node_user, build: isl.ast_build) -> isl.ast_node:\n",
    "    access_global = call_from(build, OpKind.Access, access_global_pma.tuple_name(\n",
    "        isl.dim_type.OUT), *[access_global_pma.at(i) for i in range(access_global_pma.size())])\n",
    "    access_local = call_from(build, OpKind.Access, access_local_pma.tuple_name(\n",
    "        isl.dim_type.OUT), *[access_local_pma.at(i) for i in range(access_local_pma.size())])\n",
    "\n",
    "    call = call_from(build, OpKind.Assign if access.buffer.usage ==\n",
    "                     UsageKind.Input else OpKind.AssertEqual, access_local, access_global)\n",
    "    return isl.ast_node_user(call)\n",
    "\n",
    "  def at_each_for(node: isl.ast_node_for, build: isl.ast_build) -> isl.ast_node:\n",
    "    it = node.get_iterator()\n",
    "    if isinstance(it, isl.ast_expr_id) and it.id().name() in builtin_iters:\n",
    "      node = node.set_annotation(IterKind.Distributed.name)\n",
    "    return node\n",
    "\n",
    "  ast_build = ast_build.set_at_each_domain(at_each_domain)\n",
    "  ast_build = ast_build.set_after_each_for(at_each_for)\n",
    "  ast_build = ast_build.set_iterators(\n",
    "      '(' + ', '.join(map(str, chain(self.mesh.dims, access.buffer.dims))) + ')')\n",
    "  ast_node = ast_build.node_from_schedule_map(sharding.range().identity())\n",
    "  alloc = isl.ast_node_user(call_from(ast_build, OpKind.Alloc, access.buffer.name, *\n",
    "                                      [buffer_local_shape.at(i) for i in range(buffer_local_shape.size())]))\n",
    "  return (alloc, ast_node)\n",
    "\n",
    "with open('tmp/1.py', 'w') as f:\n",
    "  ast_shard_buffer_0 = lower_shard(s0_communicated, s0_communicated.accesses[0])\n",
    "  ast_shard_buffer_1 = lower_shard(s0_communicated, s0_communicated.accesses[1])\n",
    "  ast_shard_buffer_2 = lower_shard(s0_communicated, s0_communicated.accesses[2])\n",
    "  printer = isl.printer.to_file(f)\n",
    "  print_options = isl.ast_print_options.alloc()\n",
    "  printer.set_output_format(isl.format.C)\n",
    "  printer = ast_shard_buffer_0[0].print(printer, print_options)\n",
    "  printer = ast_shard_buffer_0[1].print(printer, print_options)\n",
    "  printer = ast_shard_buffer_1[0].print(printer, print_options)\n",
    "  printer = ast_shard_buffer_1[1].print(printer, print_options)\n",
    "  printer = ast_shard_buffer_2[0].print(printer, print_options)\n",
    "  printer = ast_shard_buffer_2[1].print(printer, print_options)\n",
    "  printer.flush()\n",
    "  \n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d25adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_python_style_item(printer: isl.printer, item: isl.ast_expr | str):\n",
    "  match item:\n",
    "    case str():\n",
    "      printer.print_str(item)\n",
    "    case isl.ast_expr():\n",
    "      expr = item\n",
    "      match expr:\n",
    "        case isl.ast_expr_op_call():\n",
    "          op_name = expr.get_op_arg(0).id().name()\n",
    "          match op_name:\n",
    "            case OpKind.Access.name:\n",
    "              print_python_style_item(printer, expr.get_op_arg(1))  # buffer name\n",
    "              printer.print_str(\"[\")\n",
    "              for i in range(2, expr.op_n_arg()):\n",
    "                print_python_style_item(printer, expr.get_op_arg(i))\n",
    "                printer.print_str(\", \"[i - expr.op_n_arg():-1])\n",
    "              printer.print_str(\"]\")\n",
    "            case OpKind.Rank.name:\n",
    "              printer.print_str(\"[\")\n",
    "              for i in range(1, expr.op_n_arg()):\n",
    "                print_python_style_item(printer, expr.get_op_arg(i))\n",
    "                printer.print_str(\", \"[i - expr.op_n_arg():-1])\n",
    "              printer.print_str(\"]\")\n",
    "            case OpKind.Assign.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(1), \" = \", expr.get_op_arg(2))\n",
    "            case OpKind.AugAssign.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(1), \" += \", expr.get_op_arg(2))\n",
    "            case OpKind.Slice.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(1), \":\", expr.get_op_arg(2))\n",
    "              if expr.op_n_arg() > 3:\n",
    "                printer.print_str(\":\")\n",
    "                print_python_style_item(printer, expr.get_op_arg(3))\n",
    "            case OpKind.Alloc.name:\n",
    "              print_python_style_item(printer, expr.get_op_arg(1))\n",
    "              printer.print_str(\" = np.zeros([\")\n",
    "              for i in range(2, expr.op_n_arg()):\n",
    "                print_python_style_item(printer, expr.get_op_arg(i))\n",
    "                printer.print_str(\", \"[i - expr.op_n_arg():-1])\n",
    "              printer.print_str(\"])\")\n",
    "            case OpKind.AssertEqual.name:\n",
    "              print_python_style_items(printer, \"assert np.allclose(\", expr.get_op_arg(1), \", \", expr.get_op_arg(2), \")\")\n",
    "            case OpKind.Trans.name:\n",
    "              printer.print_str(op_name)\n",
    "              printer.print_str('(')\n",
    "              for i in range(1, expr.op_n_arg()):\n",
    "                print_python_style_item(printer, expr.get_op_arg(i))\n",
    "                printer.print_str(\", \"[i - expr.op_n_arg():-1])\n",
    "              printer.print_str(')')\n",
    "            case OpKind.Mul.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(1), \" * \", expr.get_op_arg(2))\n",
    "            case OpKind.MatMul.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(1), \" @ \", expr.get_op_arg(2))\n",
    "            case OpKind.CommShift.name:\n",
    "              print_python_style_items(printer, expr.get_op_arg(0), \"(\")\n",
    "              print_python_style_item(printer, '(')\n",
    "              for i in range(1, expr.op_n_arg() - 1):\n",
    "                print_python_style_items(printer, expr.get_op_arg(i), ',')\n",
    "              print_python_style_item(printer, '),')\n",
    "              print_python_style_item(printer, expr.get_op_arg(expr.op_n_arg() - 1))\n",
    "              print_python_style_item(printer, ')')\n",
    "            case _:\n",
    "              printer.print_ast_expr(expr)\n",
    "        case _:\n",
    "          printer.print_ast_expr(expr)\n",
    "    case _:\n",
    "      raise NotImplementedError()\n",
    "\n",
    "\n",
    "def print_python_style_items(printer: isl.printer, *items: List[isl.ast_expr | str]):\n",
    "  for item in items:\n",
    "    print_python_style_item(printer, item)\n",
    "\n",
    "\n",
    "def print_python_style_user(printer: isl.printer, options: isl.ast_print_options, node: isl.ast_node):\n",
    "  expr: isl.ast_expr = node.get_expr()\n",
    "  printer.start_line()\n",
    "  print_python_style_item(printer, expr)\n",
    "  printer.end_line()\n",
    "  return printer\n",
    "\n",
    "\n",
    "def print_python_style_for(printer: isl.printer, options: isl.ast_print_options, node: isl.ast_node_for):\n",
    "  (it, init, cond, inc) = node.get_iterator(), node.get_init(), node.get_cond(), node.get_inc()\n",
    "  omit = False\n",
    "  try:\n",
    "    anno = node.annotation()\n",
    "    if anno.name() == IterKind.Distributed.name:\n",
    "      omit = True\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  if not omit:\n",
    "    printer.start_line()\n",
    "    print_python_style_items(printer, \"for \", it, \" in range(\", init,\n",
    "                             \", \", cond.get_arg(1), \" + 1\", \", \", inc, \"):\\n\")\n",
    "    printer = printer.indent(4)\n",
    "  printer = node.get_body().print(printer, options)\n",
    "  if not omit:\n",
    "    printer = printer.indent(-4)\n",
    "  return printer\n",
    "\n",
    "\n",
    "def print_python_style_block(printer: isl.printer, options: isl.ast_print_options, node: isl.ast_node_block):\n",
    "  children = node.get_children()\n",
    "  for i in range(children.size()):\n",
    "    printer = children.get_at(i).print(printer, options)\n",
    "  return printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4325608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ko in range(0, 7 + 1, 1):\n",
      "    TransB = np.zeros([256,128])\n",
      "    Trans(CommBroadcast(0),B[0:256,0:128],[ko],TransB[0:256,0:128],[x])\n",
      "    TransA = np.zeros([64,256])\n",
      "    Trans(CommBroadcast(1),A[0:64,0:256],[ko],TransA[0:64,0:256],[y])\n",
      "    C[0:64,0:128] += TransA[0:64,0:256] @ TransB[0:256,0:128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/1.py', 'w') as f:\n",
    "  ast_computation = lower_computation(s0_tensorized)\n",
    "  printer = isl.printer.to_file(f)\n",
    "  print_options = isl.ast_print_options.alloc()\n",
    "  print_options = print_options.set_print_for(print_python_style_for)\n",
    "  print_options = print_options.set_print_user(print_python_style_user)\n",
    "  print_options = print_options.set_print_block(print_python_style_block)\n",
    "  printer.set_output_format(isl.format.C)\n",
    "  printer = ast_computation.print(printer, print_options)\n",
    "  printer.flush()\n",
    "\n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679c62d",
   "metadata": {},
   "source": [
    "在codegen的过程中，通信操作我使用统一的api，包括这些api以及一些setup的代码，也需要生成出来。不过并不包含在compile的过程中。如果采用schedule tree进行代码生成，可以采用先插入mark节点后续callback处理的的方法正确生成setup相关代码，本教程为了简单起见，采用直接构造字符串写入文件的方式来处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52544aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codegen_setup(self: Computation, printer: isl.printer) -> isl.printer:\n",
    "  printer.print_str('import numpy as np\\n')\n",
    "  printer.print_str('from mpi4py import MPI\\n')\n",
    "  printer.print_str('from enum import IntEnum\\n')\n",
    "  printer.print_str('import sys\\n')\n",
    "  printer.print_str('RANK = MPI.COMM_WORLD.Get_rank()\\n')\n",
    "  printer.print_str(f'MESH = [{\",\".join(map(str, self.mesh.shape))}]\\n')\n",
    "  printer.print_str(f'COMM_ALL = MPI.COMM_WORLD.Create_cart(MESH)\\n')\n",
    "  printer.print_str(f\"({','.join(map(str, self.mesh.dims))}) = PIDS = COMM_ALL.Get_coords(RANK)\\n\")\n",
    "\n",
    "  printer.print_str(f\"\"\"\n",
    "\n",
    "class CommBroadcast:\n",
    "  def __init__(self, *axes: int):\n",
    "    self.axes: tuple[int] = axes\n",
    "\n",
    "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
    "    comm = COMM_ALL.Sub([True if i in self.axes else False for i in range(len(PIDS))])\n",
    "    src_rank = comm.Get_cart_rank(source)\n",
    "    if comm.Get_rank() == src_rank:\n",
    "      np.copyto(recvbuf, srcbuf)\n",
    "    comm.Bcast(recvbuf, root=src_rank)\n",
    "\n",
    "class CommShift:\n",
    "  def __init__(self, axes: tuple[int], delta: int):\n",
    "    self.axes: tuple[int] = axes\n",
    "    self.delta: int = delta\n",
    "\n",
    "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
    "    diffs = [p[0] - p[1] for p in zip(source, dest)]\n",
    "    if not all([diffs[axis] == 0 for axis in self.axes]):\n",
    "      src_rank = COMM_ALL.Get_cart_rank(\n",
    "          [(cord - self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
    "      dest_rank = COMM_ALL.Get_cart_rank(\n",
    "          [(cord + self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
    "      COMM_ALL.Sendrecv_replace(srcbuf, dest=dest_rank, source=src_rank)\n",
    "    np.copyto(recvbuf, srcbuf)\n",
    "\n",
    "\n",
    "class CommSendrecv:\n",
    "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
    "    COMM_ALL.Sendrecv(srcbuf, COMM_ALL.Get_cart_rank(dest), 0,\n",
    "                      recvbuf, COMM_ALL.Get_cart_rank(source))\n",
    "\n",
    "def Trans(comm, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
    "  comm(srcbuf, source, recvbuf, dest)\n",
    "\n",
    "\"\"\")\n",
    "  return printer\n",
    "\n",
    "\n",
    "def codegen_computation(self: Computation, printer: isl.printer, ast_computation: isl.ast_node):\n",
    "  printer.print_str(f\"def computation({', '.join([a.buffer.name for a in self.accesses])}):\\n\")\n",
    "  printer.set_indent(4)\n",
    "  printer = ast_computation.print(printer, print_options)\n",
    "  printer.set_indent(-4)\n",
    "  printer.end_line()\n",
    "  return printer\n",
    "\n",
    "\n",
    "def codegen_main(self: Computation, printer: isl.printer, ast_inputs: List[isl.ast_node], ast_outputs: List[isl.ast_node]):\n",
    "  printer.print_str('if __name__ == \"__main__\":\\n')\n",
    "  for i, access in enumerate(self.accesses):\n",
    "    printer.print_str(\n",
    "        f'    Global{access.buffer.name} = np.load(sys.argv[{i + 1}], mmap_mode=\"r\")\\n')\n",
    "\n",
    "  printer.set_indent(4)\n",
    "  for in_ast in ast_inputs:\n",
    "    printer = in_ast[0].print(printer, print_options)\n",
    "    printer = in_ast[1].print(printer, print_options)\n",
    "  for out_ast in ast_outputs:\n",
    "    printer = out_ast[0].print(printer, print_options)\n",
    "  printer.print_str(f\"    computation({', '.join([a.buffer.name for a in self.accesses])})\\n\")\n",
    "  for out_ast in ast_outputs:\n",
    "    printer = out_ast[1].print(printer, print_options)\n",
    "  printer.set_indent(-4)\n",
    "  printer.end_line()\n",
    "  return printer\n",
    "\n",
    "\n",
    "def codegen_full(self: Computation, f: FileIO, ast_inputs: List[isl.ast_node], ast_outputs: List[Tuple[isl.ast_node, isl.ast_node]], ast_computation: isl.ast_node):\n",
    "  printer = isl.printer.to_file(f)\n",
    "  print_options = isl.ast_print_options.alloc()\n",
    "  print_options = print_options.set_print_for(print_python_style_for)\n",
    "  print_options = print_options.set_print_user(print_python_style_user)\n",
    "  print_options = print_options.set_print_block(print_python_style_block)\n",
    "  printer.set_output_format(isl.format.C)\n",
    "\n",
    "  printer = codegen_setup(self, printer)\n",
    "  printer = codegen_computation(self, printer, ast_computation)\n",
    "  printer = codegen_main(self, printer, ast_inputs, ast_outputs)\n",
    "\n",
    "  printer.flush()\n",
    "\n",
    "\n",
    "def lower_and_codegen(self: Computation, f: FileIO):\n",
    "  ast_shard_inputs = [lower_shard(self, access)\n",
    "                      for access in self.accesses if access.buffer.usage == UsageKind.Input]\n",
    "  ast_shard_outputs = [lower_shard(self, access)\n",
    "                       for access in self.accesses if access.buffer.usage == UsageKind.Output]\n",
    "  ast_computation = lower_computation(self)\n",
    "  codegen_full(self, f, ast_shard_inputs, ast_shard_outputs, ast_computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7e25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from mpi4py import MPI\n",
      "from enum import IntEnum\n",
      "import sys\n",
      "RANK = MPI.COMM_WORLD.Get_rank()\n",
      "MESH = [8,8]\n",
      "COMM_ALL = MPI.COMM_WORLD.Create_cart(MESH)\n",
      "(x,y) = PIDS = COMM_ALL.Get_coords(RANK)\n",
      "\n",
      "\n",
      "class CommBroadcast:\n",
      "  def __init__(self, *axes: int):\n",
      "    self.axes: tuple[int] = axes\n",
      "\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    comm = COMM_ALL.Sub([True if i in self.axes else False for i in range(len(PIDS))])\n",
      "    src_rank = comm.Get_cart_rank(source)\n",
      "    if comm.Get_rank() == src_rank:\n",
      "      np.copyto(recvbuf, srcbuf)\n",
      "    comm.Bcast(recvbuf, root=src_rank)\n",
      "\n",
      "class CommShift:\n",
      "  def __init__(self, axes: tuple[int], delta: int):\n",
      "    self.axes: tuple[int] = axes\n",
      "    self.delta: int = delta\n",
      "\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    diffs = [p[0] - p[1] for p in zip(source, dest)]\n",
      "    if not all([diffs[axis] == 0 for axis in self.axes]):\n",
      "      src_rank = COMM_ALL.Get_cart_rank(\n",
      "          [(cord - self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
      "      dest_rank = COMM_ALL.Get_cart_rank(\n",
      "          [(cord + self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
      "      COMM_ALL.Sendrecv_replace(srcbuf, dest=dest_rank, source=src_rank)\n",
      "    np.copyto(recvbuf, srcbuf)\n",
      "\n",
      "\n",
      "class CommSendrecv:\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    COMM_ALL.Sendrecv(srcbuf, COMM_ALL.Get_cart_rank(dest), 0,\n",
      "                      recvbuf, COMM_ALL.Get_cart_rank(source))\n",
      "\n",
      "def Trans(comm, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "  comm(srcbuf, source, recvbuf, dest)\n",
      "\n",
      "def computation(C, A, B):\n",
      "    for ko in range(0, 7 + 1, 1):\n",
      "        TransB = np.zeros([256,128])\n",
      "        Trans(CommBroadcast(0),B[0:256,0:128],[ko],TransB[0:256,0:128],[x])\n",
      "        TransA = np.zeros([64,256])\n",
      "        Trans(CommBroadcast(1),A[0:64,0:256],[ko],TransA[0:64,0:256],[y])\n",
      "        C[0:64,0:128] += TransA[0:64,0:256] @ TransB[0:256,0:128]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    GlobalC = np.load(sys.argv[1], mmap_mode=\"r\")\n",
      "    GlobalA = np.load(sys.argv[2], mmap_mode=\"r\")\n",
      "    GlobalB = np.load(sys.argv[3], mmap_mode=\"r\")\n",
      "    C = np.zeros([64,128])\n",
      "    for m in range(0, 63 + 1, 1):\n",
      "        for n in range(0, 127 + 1, 1):\n",
      "            assert np.allclose(C[m,n], GlobalC[64 * x + m,128 * y + n])\n",
      "    A = np.zeros([64,256])\n",
      "    for m in range(0, 63 + 1, 1):\n",
      "        for k in range(0, 255 + 1, 1):\n",
      "            A[m,k] = GlobalA[64 * x + m,256 * y + k]\n",
      "    B = np.zeros([256,128])\n",
      "    computation(C, A, B)\n",
      "    for k in range(0, 255 + 1, 1):\n",
      "        for n in range(0, 127 + 1, 1):\n",
      "            B[k,n] = GlobalB[256 * x + k,128 * y + n]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"tmp/1.py\", \"w\") as f:\n",
    "  codegen_full(s0_tensorized, f, [ast_shard_buffer_0, ast_shard_buffer_1], [ast_shard_buffer_2], ast_computation)\n",
    "\n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636fa1",
   "metadata": {},
   "source": [
    "开始创建输入并执行程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arrA = np.random.rand(512, 2048).astype(np.float32)\n",
    "arrB = np.random.rand(2048, 1024).astype(np.float32)\n",
    "np.save(\"tmp/arrA.npy\", arrA)\n",
    "np.save(\"tmp/arrB.npy\", arrB)\n",
    "np.save(\"tmp/arrC.npy\", arrA @ arrB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arrA = np.arange(16 * 64).astype(np.float32).reshape([16, 64])\n",
    "arrB = np.arange(64 * 32).astype(np.float32).reshape([64, 32])\n",
    "# arrB = np.ones_like(arrB)\n",
    "np.save(\"tmp/arrA.npy\", arrA)\n",
    "np.save(\"tmp/arrB.npy\", arrB)\n",
    "np.save(\"tmp/arrC.npy\", arrA @ arrB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55502d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mpirun --map-by :OVERSUBSCRIBE -n 64 python tmp/1.py tmp/arrA.npy tmp/arrB.npy tmp/arrC.npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computation(op=<OpKind.MatMul: 131>, domain=isl.set(\"{ s0[k, m, n] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 }\"), schedule=isl.map(\"{ s0[k, m, n] -> s0[mo, no, ko, mi = m - 64mo, ni = n - 128no, ki = k - 256ko] : 0 <= k <= 2047 and 0 <= m <= 511 and 0 <= n <= 1023 and -63 + m <= 64mo <= m and -127 + n <= 128no <= n and -255 + k <= 256ko <= k }\"), accesses=(Access(buffer=Buffer(name='C', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ C[m, n] -> C[x, y, m - 64x, local_n = n - 128y] : 0 <= m <= 511 and 0 <= n <= 1023 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Output: 1>), relation=isl.map(\"{ s0[k, m, n] -> C[m, n] }\")), Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\"))), iter_vars=(IterVar(name='mo', lower_bound=None, upper_bound=None, step=1), IterVar(name='no', lower_bound=None, upper_bound=None, step=1), IterVar(name='ko', lower_bound=None, upper_bound=None, step=1), IterVar(name='mi', lower_bound=None, upper_bound=None, step=1), IterVar(name='ni', lower_bound=None, upper_bound=None, step=1), IterVar(name='ki', lower_bound=None, upper_bound=None, step=1)), mesh=Mesh(dims=(IterVar(name='x', lower_bound=0, upper_bound=8, step=1), IterVar(name='y', lower_bound=0, upper_bound=8, step=1))), iter_kinds={IterVar(name='mo', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='no', lower_bound=None, upper_bound=None, step=1): <IterKind.Distributed: 1>, IterVar(name='mi', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>, IterVar(name='ni', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>, IterVar(name='ki', lower_bound=None, upper_bound=None, step=1): <IterKind.Tensorize: 2>}, transfers={IterVar(name='ko', lower_bound=None, upper_bound=None, step=1): (Transfer(access=Access(buffer=Buffer(name='B', dims=(IterVar(name='k', lower_bound=0, upper_bound=2048, step=1), IterVar(name='n', lower_bound=0, upper_bound=1024, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ B[k, n] -> B[x, y, k - 256x, local_n = n - 128y] : 0 <= k <= 2047 and 0 <= n <= 1023 and 0 <= x <= 7 and -255 + k <= 256x <= k and 0 <= y <= 7 and -127 + n <= 128y <= n }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> B[k, n] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")), Transfer(access=Access(buffer=Buffer(name='A', dims=(IterVar(name='m', lower_bound=0, upper_bound=512, step=1), IterVar(name='k', lower_bound=0, upper_bound=2048, step=1)), dtype=Float32Type(), sharding=isl.map(\"{ A[m, k] -> A[x, y, m - 64x, local_k = k - 256y] : 0 <= m <= 511 and 0 <= k <= 2047 and 0 <= x <= 7 and -63 + m <= 64x <= m and 0 <= y <= 7 and -255 + k <= 256y <= k }\"), usage=<UsageKind.Input: 0>), relation=isl.map(\"{ s0[k, m, n] -> A[m, k] }\")), schedule=isl.map(\"{ s0[mo, no, ko, mi, ni, ki] -> s0[mo' = mo, no' = no, ko' = ko, mi' = mi, ni' = ni, ki' = ki] : mi >= 0 and -64mo <= mi <= 511 - 64mo and mi <= 63 and ni >= 0 and -128no <= ni <= 1023 - 128no and ni <= 127 and ki >= 0 and -256ko <= ki <= 2047 - 256ko and ki <= 255 }\")))})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0_tensorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453e197",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Computation' object has no attribute 'tensorize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m s0_sharded = s0_distributed.shard(\u001b[33m'\u001b[39m\u001b[33mA\u001b[39m\u001b[33m'\u001b[39m, m @ x, k @ y). \\\n\u001b[32m      9\u001b[39m     shard(\u001b[33m'\u001b[39m\u001b[33mB\u001b[39m\u001b[33m'\u001b[39m, k @ x, n @ y). \\\n\u001b[32m     10\u001b[39m     shard(\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m, m @ x, n @ y)\n\u001b[32m     12\u001b[39m s0_communicated = s0_sharded.communicate(\u001b[33m'\u001b[39m\u001b[33mA\u001b[39m\u001b[33m'\u001b[39m, ko, [no]).communicate(\u001b[33m'\u001b[39m\u001b[33mB\u001b[39m\u001b[33m'\u001b[39m, ko)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m s0_tensorized = \u001b[43ms0_communicated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensorize\u001b[49m([mi, ni, ki], OpKind.MatMul)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Computation' object has no attribute 'tensorize'"
     ]
    }
   ],
   "source": [
    "x, y = IterVar.range('x', 8), IterVar.range('y', 8)\n",
    "mo, no, mi, ni = IterVar.symbol('mo no mi ni')\n",
    "ko, ki = IterVar.symbol('ko ki')\n",
    "s0_distributed = s0.distribute([m, n], [mo, no], [mi, ni], Mesh((x, y))). \\\n",
    "    divide(k, ko, ki, x.extent). \\\n",
    "    reorder(mo, no, ko, mi, ni, ki)\n",
    "\n",
    "s0_sharded = s0_distributed.shard('A', m @ x, k @ y). \\\n",
    "    shard('B', k @ x, n @ y). \\\n",
    "    shard('C', m @ x, n @ y)\n",
    "\n",
    "s0_communicated = s0_sharded.communicate('A', ko, [no]).communicate('B', ko)\n",
    "\n",
    "s0_tensorized = s0_communicated.tensorize([mi, ni, ki], OpKind.MatMul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae7ddb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from mpi4py import MPI\n",
      "from enum import IntEnum\n",
      "import sys\n",
      "RANK = MPI.COMM_WORLD.Get_rank()\n",
      "MESH = [8,8]\n",
      "COMM_ALL = MPI.COMM_WORLD.Create_cart(MESH)\n",
      "(x,y) = PIDS = COMM_ALL.Get_coords(RANK)\n",
      "\n",
      "\n",
      "class CommBroadcast:\n",
      "  def __init__(self, *axes: int):\n",
      "    self.axes: tuple[int] = axes\n",
      "\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    comm = COMM_ALL.Sub([True if i in self.axes else False for i in range(len(PIDS))])\n",
      "    src_rank = comm.Get_cart_rank(source)\n",
      "    if comm.Get_rank() == src_rank:\n",
      "      np.copyto(recvbuf, srcbuf)\n",
      "    comm.Bcast(recvbuf, root=src_rank)\n",
      "\n",
      "class CommShift:\n",
      "  def __init__(self, axes: tuple[int], delta: int):\n",
      "    self.axes: tuple[int] = axes\n",
      "    self.delta: int = delta\n",
      "\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    diffs = [p[0] - p[1] for p in zip(source, dest)]\n",
      "    if not all([diffs[axis] == 0 for axis in self.axes]):\n",
      "      src_rank = COMM_ALL.Get_cart_rank(\n",
      "          [(cord - self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
      "      dest_rank = COMM_ALL.Get_cart_rank(\n",
      "          [(cord + self.delta) % MESH[i] if i in self.axes else cord for i, cord in enumerate(dest)])\n",
      "      COMM_ALL.Sendrecv_replace(srcbuf, dest=dest_rank, source=src_rank)\n",
      "    np.copyto(recvbuf, srcbuf)\n",
      "\n",
      "\n",
      "class CommSendrecv:\n",
      "  def __call__(self, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "    COMM_ALL.Sendrecv(srcbuf, COMM_ALL.Get_cart_rank(dest), 0,\n",
      "                      recvbuf, COMM_ALL.Get_cart_rank(source))\n",
      "\n",
      "def Trans(comm, srcbuf: np.ndarray, source: list[int], recvbuf: np.ndarray, dest: list[int]):\n",
      "  comm(srcbuf, source, recvbuf, dest)\n",
      "\n",
      "def computation(C, A, B):\n",
      "    for ko in range(0, 7 + 1, 1):\n",
      "        TransB = np.zeros([256,128])\n",
      "        Trans(CommBroadcast(0),B[0:256,0:128],[ko],TransB[0:256,0:128],[x])\n",
      "        TransA = np.zeros([64,256])\n",
      "        Trans(CommShift((1,),1),A[0:64,0:256],[x,(y + ko) % 8],TransA[0:64,0:256],[x,y])\n",
      "        C[0:64,0:128] += TransA[0:64,0:256] @ TransB[0:256,0:128]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    GlobalC = np.load(sys.argv[1], mmap_mode=\"r\")\n",
      "    GlobalA = np.load(sys.argv[2], mmap_mode=\"r\")\n",
      "    GlobalB = np.load(sys.argv[3], mmap_mode=\"r\")\n",
      "    A = np.zeros([64,256])\n",
      "    for m in range(0, 63 + 1, 1):\n",
      "        for k in range(0, 255 + 1, 1):\n",
      "            A[m,k] = GlobalA[64 * x + m,256 * y + k]\n",
      "    B = np.zeros([256,128])\n",
      "    for k in range(0, 255 + 1, 1):\n",
      "        for n in range(0, 127 + 1, 1):\n",
      "            B[k,n] = GlobalB[256 * x + k,128 * y + n]\n",
      "    C = np.zeros([64,128])\n",
      "    computation(C, A, B)\n",
      "    for m in range(0, 63 + 1, 1):\n",
      "        for n in range(0, 127 + 1, 1):\n",
      "            assert np.allclose(C[m,n], GlobalC[64 * x + m,128 * y + n])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"tmp/1.py\", \"w\") as f:\n",
    "  lower_and_codegen(s0_tensorized, f)\n",
    "\n",
    "with open('tmp/1.py', 'r') as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96529b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
